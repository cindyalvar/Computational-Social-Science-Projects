{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Computational Social Science] Project 5: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you will use natural language processing techniques to explore a dataset containing tweets from members of the 116th United States Congress that met from January 3, 2019 to January 2, 2021. The dataset has also been cleaned to contain information about each legislator. Concretely, you will do the following:\n",
    "\n",
    "* Preprocess the text of legislators' tweets\n",
    "* Conduct Exploratory Data Analysis of the text\n",
    "* Use sentiment analysis to explore differences between legislators' tweets\n",
    "* Featurize text with manual feature engineering, frequency-based, and vector-based techniques\n",
    "* Predict legislators' political parties and whether they are a Senator or Representative\n",
    "\n",
    "You will explore two questions that relate to two central findings in political science and examine how they relate to the text of legislators' tweets. First, political scientists have argued that U.S. politics is currently highly polarized relative to other periods in American history, but also that the polarization is asymmetric. Historically, there were several conservative Democrats (i.e. \"blue dog Democrats\") and liberal Republicans (i.e. \"Rockefeller Republicans\"), as measured by popular measurement tools like [DW-NOMINATE](https://en.wikipedia.org/wiki/NOMINATE_(scaling_method)#:~:text=DW\\%2DNOMINATE\\%20scores\\%20have\\%20been,in\\%20the\\%20liberal\\%2Dconservative\\%20scale.). However, in the last few years, there are few if any examples of any Democrat in Congress being further to the right than any Republican and vice versa. At the same time, scholars have argued that this polarization is mostly a function of the Republican party moving further right than the Democratic party has moved left. **Does this sort of asymmetric polarization show up in how politicians communicate to their constituents through tweets?**\n",
    "\n",
    "Second, the U.S. Congress is a bicameral legislature, and there has long been debate about partisanship in the Senate versus the House. The House of Representatives is apportioned by population and all members serve two year terms. In the Senate, each state receives two Senators and each Senator serves a term of six years. For a variety of reasons (smaller chamber size, more insulation from the voters, rules and norms like the filibuster, etc.), the Senate has been argued to be the \"cooling saucer\" of Congress in that it is more bipartisan and moderate than the House. **Does the theory that the Senate is more moderate have support in Senators' tweets?**\n",
    "\n",
    "**Note**: See the project handout for more details on caveats and the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# emoji for preprocessing\n",
    "#!pip install emoji\n",
    "import emoji\n",
    "\n",
    "# punctuation, stop words and English language model\n",
    "from string import punctuation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# countvectorizer, tfidfvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "# ----------\n",
    "congress_tweets = pd.read_csv(\"data/116th Congressional Tweets and Demographics.csv\")\n",
    "# fill in this line of code with a sufficient number of tweets, depending on your computational resources\n",
    "congress_tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## limiting sample to 5000 tweets (large enough to hopefully catch most oddities) \n",
    "congress_tweets_sample = congress_tweets.sample(n=5000)\n",
    "\n",
    "#visualizing column\n",
    "congress_tweets_sample['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in working with text data is to preprocess it. Make sure you do the following:\n",
    "\n",
    "* Remove punctuation and stop words. The `rem_punc_stop()` function we used in lab is provided to you but you should feel free to edit it as necessary for other steps\n",
    "* Remove tokens that occur frequently in tweets, but may not be helpful for downstream classification. For instance, many tweets contain a flag for retweeting, or share a URL \n",
    "\n",
    "As you search online, you might run into solutions that rely on regular expressions. You are free to use these, but you should also be able to preprocess using the techniques we covered in lab. Specifically, we encourage you to use spaCy's token attributes and string methods to do some of this text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_punc_stop(text):\n",
    "    # set objects\n",
    "    stop_words = STOP_WORDS\n",
    "    additional_stops = {'u', 'e', 'eHouse'} # I identified these after creating a few wordclouds iteratively\n",
    "    stop_words.update(additional_stops)\n",
    "    punc = set(punctuation)\n",
    "    \n",
    "    # First split into words\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(words):\n",
    "        # Remove RT/QT and skip the next word (username) if it's a retweet/quote tweet pattern\n",
    "        if (i < len(words) - 1 and \n",
    "            (words[i] == \"RT\" or words[i] == \"QT\") and \n",
    "            words[i + 1].startswith(\"@\")):\n",
    "            i += 2  # Skip both RT/QT and the @username\n",
    "        # Remove any @username mentions\n",
    "        elif words[i].startswith(\"@\"):\n",
    "            i += 1  # Skip the @username\n",
    "        # Remove standalone RT, QT\n",
    "        elif words[i] in [\"RT\", \"QT\"]:\n",
    "            i += 1\n",
    "        else:\n",
    "            cleaned_words.append(words[i])\n",
    "            i += 1\n",
    "    \n",
    "    # Join back into text\n",
    "    text = \" \".join(cleaned_words)\n",
    "    \n",
    "    # remove punctuation\n",
    "    punc_free = \"\".join([ch for ch in text if ch not in punc])\n",
    "    \n",
    "    # remove emojis using emoji library\n",
    "    punc_free = emoji.replace_emoji(punc_free, '')\n",
    "    \n",
    "    # remove URL\n",
    "    words = punc_free.split()\n",
    "    punc_free = \" \".join([word for word in words if not word.startswith(('http', 'https', 'www'))])\n",
    "    \n",
    "    # Remove extra whitespace created by removals\n",
    "    punc_free = \" \".join(punc_free.split())\n",
    "    \n",
    "    # apply nlp to punctuation-free object\n",
    "    doc = nlp(punc_free)\n",
    "    \n",
    "    # extract words from processed text\n",
    "    spacy_words = [token.text for token in doc]\n",
    "    \n",
    "    # filter out words\n",
    "    spacy_words = [word for word in spacy_words if not word.startswith('http')]\n",
    "    \n",
    "    # Remove 'amp' from the word list (identified after initial cleaning and making a wordcloud; doubled back to remove\n",
    "    no_punc = [word for word in spacy_words if word not in stop_words and word != 'amp']\n",
    "    \n",
    "    # return\n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on one tweet\n",
    "sample_tweet = congress_tweets_sample['text'].iloc[0]\n",
    "print(\"Original:\", sample_tweet)\n",
    "print(\"Processed:\", rem_punc_stop(sample_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to the 'text' column in the sample rows from the dataset\n",
    "congress_tweets_sample['tokens'] = congress_tweets_sample['text'].map(lambda x: rem_punc_stop(x))\n",
    "\n",
    "# visualize\n",
    "congress_tweets_sample['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use two of the techniques we covered in lab (or other techniques outside of lab!) to explore the text of the tweets. You should construct these visualizations with an eye toward the eventual classification tasks: (1) predicting the legislator's political party based on the text of their tweet, and (2) predicting whether the legislator is a Senator or Representative. As a reminder, in lab we covered word frequencies, word clouds, word/character counts, scattertext, and topic modeling as possible exploration tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "# ----------\n",
    "# apply function to text object\n",
    "text = ' '.join(congress_tweets_sample['tokens'].map(lambda text: ' '.join(text)))\n",
    "\n",
    "# create WordCloud visualization using the \"text\" object \n",
    "wordcloud = WordCloud(random_state=40).generate(text) # set random state to ensure same word cloud each time\n",
    "\n",
    "# plot \n",
    "plt.imshow(wordcloud,                  # specify wordcloud\n",
    "           interpolation = 'bilinear') # specifies how the words are displayed\n",
    "plt.axis('off')                        # turn off axes\n",
    "plt.show()                             # show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for individuals whose party is republican\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# Filter the dataframe to include only Republican party members\n",
    "republican_tweets = congress_tweets_sample[congress_tweets_sample['party'] == 'Republican']\n",
    "\n",
    "# Join the tokens for Republican tweets\n",
    "republican_text = ' '.join(republican_tweets['tokens'].map(lambda text: ' '.join(text)))\n",
    "\n",
    "# Create WordCloud visualization for Republican tweets\n",
    "republican_wordcloud = WordCloud(random_state=40).generate(republican_text)  # set random state to ensure same word cloud each time\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))  # Optional: set figure size for better visibility\n",
    "plt.imshow(republican_wordcloud, \n",
    "           interpolation = 'bilinear')  # specifies how the words are displayed\n",
    "plt.axis('off')                        # turn off axes\n",
    "plt.title('Word Cloud - Republican Party Tweets')  # Optional: add a title\n",
    "plt.show()                             # show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for individuals whose party is democrat\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# Filter the dataframe to include only Republican party members\n",
    "democrat_tweets = congress_tweets_sample[congress_tweets_sample['party'] == 'Democrat']\n",
    "\n",
    "# Join the tokens for Republican tweets\n",
    "democrat_text = ' '.join(democrat_tweets['tokens'].map(lambda text: ' '.join(text)))\n",
    "\n",
    "# Create WordCloud visualization for Democrat tweets\n",
    "democrat_wordcloud = WordCloud(random_state=40).generate(democrat_text)  # set random state to ensure same word cloud each time\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))  # Optional: set figure size for better visibility\n",
    "plt.imshow(democrat_wordcloud, \n",
    "           interpolation = 'bilinear')  # specifies how the words are displayed\n",
    "plt.axis('off')                        # turn off axes\n",
    "plt.title('Word Cloud - Democratic Party Tweets')  # Optional: add a title\n",
    "plt.show()                             # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lengths and counts for overall tweets\n",
    "# create new feature columns with counts of # of characters and # of words\n",
    "# ---------------\n",
    "# count of number of characters\n",
    "congress_tweets_sample['tweet_char'] = congress_tweets_sample['text'].apply(len)\n",
    "# count of number of words\n",
    "congress_tweets_sample['word_count'] = congress_tweets_sample['text'].apply(lambda x: len(str(x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histogram of general tweet character length (number of characters)\n",
    "sns.displot(congress_tweets_sample, # specify data\n",
    "            x=\"tweet_char\")         # x-axis feature\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histogram of general word count \n",
    "sns.displot(congress_tweets_sample, # specify data\n",
    "            x=\"word_count\")         # x-axis feature\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create historgrams for Democrats and Republicans\n",
    "# ----\n",
    "# Filter data for Republicans and Democrats\n",
    "republican_tweets = congress_tweets_sample[congress_tweets_sample['party'] == 'Republican']\n",
    "democrat_tweets = congress_tweets_sample[congress_tweets_sample['party'] == 'Democrat']\n",
    "\n",
    "# Create figure with subplots - 2 rows (character length, word count) and 2 columns (Republican, Democrat)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Character length histograms\n",
    "sns.histplot(republican_tweets, x=\"tweet_char\", ax=axes[0, 0], color='red')\n",
    "axes[0, 0].set_title('Republican Tweet Character Length')\n",
    "axes[0, 0].set_xlabel('Number of Characters')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "sns.histplot(democrat_tweets, x=\"tweet_char\", ax=axes[0, 1], color='blue')\n",
    "axes[0, 1].set_title('Democrat Tweet Character Length')\n",
    "axes[0, 1].set_xlabel('Number of Characters')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Word count histograms\n",
    "sns.histplot(republican_tweets, x=\"word_count\", ax=axes[1, 0], color='red')\n",
    "axes[1, 0].set_title('Republican Tweet Word Count')\n",
    "axes[1, 0].set_xlabel('Number of Words')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "sns.histplot(democrat_tweets, x=\"word_count\", ax=axes[1, 1], color='blue')\n",
    "axes[1, 1].set_title('Democrat Tweet Word Count')\n",
    "axes[1, 1].set_xlabel('Number of Words')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's analyze the sentiments contained within the tweets. You may use TextBlob or another library for these tasks. Do the following:\n",
    "\n",
    "* Choose two legislators, one who you think will be more liberal and one who you think will be more conservative, and analyze their sentiment and/or subjectivity scores per tweet. For instance, you might do two scatterplots that plot each legislator's sentiment against their subjectivity, or two density plots for their sentiments. Do the scores match what you thought?\n",
    "* Plot two more visualizations like the ones you chose in the first part, but do them to compare (1) Democrats v. Republicans and (2) Senators v. Representatives \n",
    "\n",
    "`TextBlob` has already been imported in the top cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature colum of sentiment polarity\n",
    "# ---------- \n",
    "# create the \"tokens\" column again \n",
    "congress_tweets_sample['tokens'] = congress_tweets_sample['tokens'].map(lambda text: ' '.join(text))\n",
    "\n",
    "# create \"polarity\" column \n",
    "congress_tweets_sample['polarity'] = congress_tweets_sample['tokens'].map(lambda text: TextBlob(text).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for subjectivity\n",
    "# ------\n",
    "congress_tweets_sample['subjectivity'] = congress_tweets_sample['tokens'].map(lambda text: TextBlob(text).sentiment.subjectivity)\n",
    "\n",
    "# view\n",
    "congress_tweets_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity vs. subjectivity scatterplots for Bernie Sanders and Ted Cruz\n",
    "\n",
    "# Filter for Bernie Sanders and Ted Cruz based on screen_name as shown in your code\n",
    "sanders_tweets = congress_tweets_sample[congress_tweets_sample['screen_name'] == 'SenSanders']\n",
    "cruz_tweets = congress_tweets_sample[congress_tweets_sample['screen_name'] == 'SenTedCruz']\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Scatterplot for Bernie Sanders\n",
    "sns.scatterplot(\n",
    "    data=sanders_tweets,\n",
    "    x='polarity',\n",
    "    y='subjectivity',\n",
    "    ax=ax1,\n",
    "    color='blue',\n",
    "    alpha=0.7,\n",
    "    s=100  # slightly larger point size for better visibility\n",
    ")\n",
    "ax1.set_title('Bernie Sanders: Sentiment vs. Subjectivity')\n",
    "ax1.set_xlabel('Sentiment Polarity (-1 to 1)')\n",
    "ax1.set_ylabel('Subjectivity (0 to 1)')\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add text to highlight that Bernie Sanders is more liberal\n",
    "ax1.text(0.05, 0.95, \"More Liberal Representative\", transform=ax1.transAxes, \n",
    "         fontsize=12, fontweight='bold', color='blue')\n",
    "\n",
    "# Scatterplot for Ted Cruz\n",
    "sns.scatterplot(\n",
    "    data=cruz_tweets,\n",
    "    x='polarity',\n",
    "    y='subjectivity',\n",
    "    ax=ax2,\n",
    "    color='red',\n",
    "    alpha=0.7,\n",
    "    s=100  # slightly larger point size for better visibility\n",
    ")\n",
    "ax2.set_title('Ted Cruz: Sentiment vs. Subjectivity')\n",
    "ax2.set_xlabel('Sentiment Polarity (-1 to 1)')\n",
    "ax2.set_ylabel('Subjectivity (0 to 1)')\n",
    "ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add text to highlight that Ted Cruz is more conservative\n",
    "ax2.text(0.05, 0.95, \"More Conservative Representative\", transform=ax2.transAxes, \n",
    "         fontsize=12, fontweight='bold', color='red')\n",
    "\n",
    "# Ensure both plots have the same scale for fair comparison\n",
    "# Set the limits based on reasonable sentiment and subjectivity ranges\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xlim(-1, 1)  # Sentiment polarity typically ranges from -1 to 1\n",
    "    ax.set_ylim(0, 1)   # Subjectivity typically ranges from 0 to 1\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above scatterplots show that Tex Cruz's tweets appear to skew more between neutral and positive sentiment polarity, whereas Bernie Sanders' tweets appear to skew more between neutral and negative sentiment polarity. They both appear to have similar spread in subjectivity. Bernie Sanders' results are not surprising as he is known for being extremely outspoken in his negative criticisms of issues, politicians, and government overall. I was surprised that Ted Cruz's tweets tend to skew towards positive sentiment polarity, however, as I don't necessarily think of him as being a positive person, and is often highly critical of liberal politicians. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplots for republicans vs. democrats\n",
    "\n",
    "# Get unique party values to make sure we're filtering correctly\n",
    "# Uncomment this line if you need to check what values exist in the party column\n",
    "# print(congress_tweets_sample['party'].unique())\n",
    "\n",
    "# Filter the dataframe by party\n",
    "republican_tweets = congress_tweets_sample[congress_tweets_sample['party'] == 'Republican']\n",
    "democrat_tweets = congress_tweets_sample[congress_tweets_sample['party'] == 'Democrat']\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Scatterplot for Republicans\n",
    "sns.scatterplot(\n",
    "    data=republican_tweets,\n",
    "    x='polarity',\n",
    "    y='subjectivity',\n",
    "    ax=ax1,\n",
    "    color='red',\n",
    "    alpha=0.6\n",
    ")\n",
    "ax1.set_title('Republican Legislators: Sentiment vs. Subjectivity')\n",
    "ax1.set_xlabel('Sentiment Polarity (-1 to 1)')\n",
    "ax1.set_ylabel('Subjectivity (0 to 1)')\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Scatterplot for Democrats\n",
    "sns.scatterplot(\n",
    "    data=democrat_tweets,\n",
    "    x='polarity',\n",
    "    y='subjectivity',\n",
    "    ax=ax2,\n",
    "    color='blue',\n",
    "    alpha=0.6\n",
    ")\n",
    "ax2.set_title('Democrat Legislators: Sentiment vs. Subjectivity')\n",
    "ax2.set_xlabel('Sentiment Polarity (-1 to 1)')\n",
    "ax2.set_ylabel('Subjectivity (0 to 1)')\n",
    "ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatterplots for Representatives vs Senators (sentiment polarity vs. subjectivity)\n",
    "\n",
    "# Filter for Representatives and Senators based on the 'position' column\n",
    "rep_tweets = congress_tweets_sample[congress_tweets_sample['position'] == 'Rep']\n",
    "sen_tweets = congress_tweets_sample[congress_tweets_sample['position'] == 'Sen']\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Scatterplot for Representatives\n",
    "sns.scatterplot(\n",
    "    data=rep_tweets,\n",
    "    x='polarity',\n",
    "    y='subjectivity',\n",
    "    ax=ax1,\n",
    "    color='purple',\n",
    "    alpha=0.6,\n",
    "    s=80  # point size\n",
    ")\n",
    "\n",
    "# Add trendline directly to the first axis\n",
    "sns.regplot(x='polarity', y='subjectivity', data=rep_tweets, ax=ax1, \n",
    "            scatter=False, color='indigo', line_kws={'linewidth': 2})\n",
    "\n",
    "ax1.set_title('House Representatives: Sentiment vs. Subjectivity')\n",
    "ax1.set_xlabel('Sentiment Polarity (-1 to 1)')\n",
    "ax1.set_ylabel('Subjectivity (0 to 1)')\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Scatterplot for Senators\n",
    "sns.scatterplot(\n",
    "    data=sen_tweets,\n",
    "    x='polarity',\n",
    "    y='subjectivity',\n",
    "    ax=ax2,\n",
    "    color='green',\n",
    "    alpha=0.6,\n",
    "    s=80  # point size\n",
    ")\n",
    "\n",
    "# Add trendline directly to the second axis\n",
    "sns.regplot(x='polarity', y='subjectivity', data=sen_tweets, ax=ax2, \n",
    "            scatter=False, color='darkgreen', line_kws={'linewidth': 2})\n",
    "\n",
    "ax2.set_title('Senators: Sentiment vs. Subjectivity')\n",
    "ax2.set_xlabel('Sentiment Polarity (-1 to 1)')\n",
    "ax2.set_ylabel('Subjectivity (0 to 1)')\n",
    "ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Ensure both plots have the same scale for fair comparison\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xlim(-1, 1)  # Sentiment polarity typically ranges from -1 to 1\n",
    "    ax.set_ylim(0, 1)   # Subjectivity typically ranges from 0 to 1\n",
    "\n",
    "# Show average values as vertical and horizontal lines\n",
    "ax1.axvline(x=rep_tweets['polarity'].mean(), color='purple', linestyle='--', alpha=0.7)\n",
    "ax1.axhline(y=rep_tweets['subjectivity'].mean(), color='purple', linestyle='--', alpha=0.7)\n",
    "ax2.axvline(x=sen_tweets['polarity'].mean(), color='green', linestyle='--', alpha=0.7)\n",
    "ax2.axhline(y=sen_tweets['subjectivity'].mean(), color='green', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add text annotations showing the means\n",
    "ax1.text(0.05, 0.05, f\"Avg. Polarity: {rep_tweets['polarity'].mean():.3f}\\nAvg. Subjectivity: {rep_tweets['subjectivity'].mean():.3f}\", \n",
    "         transform=ax1.transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "ax2.text(0.05, 0.05, f\"Avg. Polarity: {sen_tweets['polarity'].mean():.3f}\\nAvg. Subjectivity: {sen_tweets['subjectivity'].mean():.3f}\", \n",
    "         transform=ax2.transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the figure \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going to classification, explore different featurization techniques. Create three dataframes or arrays to represent your text features, specifically:\n",
    "\n",
    "* Features engineered from your previous analysis. For example, word counts, sentiment scores, topic model etc.\n",
    "* A term frequency-inverse document frequency matrix. \n",
    "* An embedding-based featurization (like a document averaged word2vec)\n",
    "\n",
    "In the next section, you will experiment with each of these featurization techniques to see which one produces the best classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineered Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Features engineered from previous analysis\n",
    "\n",
    "# Create a dataframe with engineered features\n",
    "engineered_features = congress_tweets_sample[['tweet_char', 'word_count', 'polarity', 'subjectivity']].copy()\n",
    "\n",
    "# Add new features based on previous analysis\n",
    "\n",
    "# Ratio of characters to words (complexity measure)\n",
    "engineered_features['avg_word_length'] = congress_tweets_sample['tweet_char'] / congress_tweets_sample['word_count']\n",
    "\n",
    "# Create categorical features\n",
    "engineered_features['polarity_category'] = pd.cut(congress_tweets_sample['polarity'], \n",
    "                                                 bins=[-1, -0.5, 0, 0.5, 1], \n",
    "                                                 labels=['Very Negative', 'Negative', 'Positive', 'Very Positive'])\n",
    "\n",
    "engineered_features['subjectivity_category'] = pd.cut(congress_tweets_sample['subjectivity'],\n",
    "                                                     bins=[0, 0.33, 0.67, 1],\n",
    "                                                     labels=['Objective', 'Neutral', 'Subjective'])\n",
    "\n",
    "# Add party and position from original dataframe if available\n",
    "if 'party' in congress_tweets_sample.columns:\n",
    "    engineered_features['party'] = congress_tweets_sample['party']\n",
    "if 'position' in congress_tweets_sample.columns:\n",
    "    engineered_features['position'] = congress_tweets_sample['position']\n",
    "\n",
    "engineered_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words or Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Frequency Based featurization: TF-IDF Matrix\n",
    "# Use the already tokenized data and join tokens back into strings for TF-IDF\n",
    "# First check if we need to convert tokens to strings\n",
    "token_type = type(congress_tweets_sample['tokens'].iloc[0])\n",
    "if token_type == list:\n",
    "    # If tokens are stored as lists, join them into strings\n",
    "    text_for_tfidf = congress_tweets_sample['tokens'].apply(lambda x: ' '.join(x))\n",
    "else:\n",
    "    # If tokens are already strings, use them directly\n",
    "    text_for_tfidf = congress_tweets_sample['tokens']\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limit to top 1000 features\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_for_tfidf)\n",
    "\n",
    "# Convert to DataFrame for easier inspection\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word2Vec model from Google; OPTIONAL depending on your computational resources (the file is ~1 GB)\n",
    "# Also note that this file path assumes that the word vectors are underneath 'data'; you may wish to point to the CSS course repo and change the path\n",
    "# or move the vector file to the project repo \n",
    "\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was having trouble using the model from Google, so I decided to train Word2Vec on my on text, as we did in lab. Code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply CBOW Word2vec model to congress_tweets_sample data \n",
    "# ----------\n",
    "model = gensim.models.Word2Vec(congress_tweets_sample['tokens'],    # specify data - sentences\n",
    "                               vector_size=100,   # set embedding size at 100\n",
    "                               window=5,          # max distance between current and predicted word\n",
    "                               min_count=5,       # ignores words with freq fewer than this threshold\n",
    "                               sg=0,              # specify Continuous Bag of Words Algorithim\n",
    "                               alpha=0.025,       # learning rate\n",
    "                               epochs = 5,        # iterations\n",
    "                               seed = 1,          # set random seed (same as random_state in sklearn )\n",
    "                               batch_words=10000, # sample size \n",
    "                               workers = 1)       # set cores to 1 to ensure this embeddings are fully reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average word embeddings for a document; use examples from lab to apply this function. You can use also other techniques such as PCA and doc2vec instead.\n",
    "def document_vector(word2vec_model, doc):\n",
    "    doc = [word for word in doc if word in model.key_to_index]\n",
    "    return np.mean(model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to text\n",
    "\n",
    "# create an array for the size of the corpus\n",
    "# ----------\n",
    "# create empty list\n",
    "empty_list_embeddings_means = []\n",
    "\n",
    "# loop over each each token\n",
    "for puppy in congress_tweets_sample['tokens']: # append the vector for each document\n",
    "    empty_list_embeddings_means.append(document_vector(model.wv, puppy))\n",
    "    \n",
    "# convert the list to array\n",
    "doc_average_embeddings = np.array(empty_list_embeddings_means) \n",
    "\n",
    "# print averages\n",
    "doc_average_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either use cross-validation or partition your data with training/validation/test sets for this section. Do the following:\n",
    "\n",
    "* Choose a supervised learning algorithm such as logistic regression, random forest etc. \n",
    "* Train six models. For each of the three dataframes you created in the featurization part, train one model to predict whether the author of the tweet is a Democrat or Republican, and a second model to predict whether the author is a Senator or Representative.\n",
    "* Report the accuracy and other relevant metrics for each of these six models.\n",
    "* Choose the featurization technique associated with your best model. Combine those text features with non-text features. Train two more models: (1) A supervised learning algorithm that uses just the non-text features and (2) a supervised learning algorithm that combines text and non-text features. Report accuracy and other relevant metrics. \n",
    "\n",
    "If time permits, you are encouraged to use hyperparameter tuning or AutoML techniques like TPOT, but are not explicitly required to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Six Models with Just Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries for classification using logistic regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineered text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Democrat or Republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for logistic model\n",
    "# ------\n",
    "# Target\n",
    "lb = LabelBinarizer()\n",
    "y_et = engineered_features['party'] = lb.fit_transform(engineered_features['party'])\n",
    "\n",
    "# Features\n",
    "\n",
    "X_et_prep = engineered_features.drop(['party'], axis = 1)\n",
    "X_et = pd.get_dummies(X_et_prep)\n",
    "X_et.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split data into train+validation and test sets (80/20)\n",
    "X_train_val_et, X_test_et, y_train_val_et, y_test_et = train_test_split(\n",
    "    X_et, y_et, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Further split train+validation into train and validation sets (75/25, resulting in 60/20/20 overall)\n",
    "X_train_et, X_val_et, y_train_et, y_val_et = train_test_split(\n",
    "    X_train_val_et, y_train_val_et, test_size=0.25, random_state=42)\n",
    "\n",
    "# 3. Standardize each set separately, fitting only on training data\n",
    "scaler_et = StandardScaler()\n",
    "X_train_scaled_et = scaler_et.fit_transform(X_train_et)\n",
    "X_val_scaled_et = scaler_et.transform(X_val_et)  # Use training params\n",
    "X_test_scaled_et = scaler_et.transform(X_test_et)  # Use training params\n",
    "\n",
    "# Convert back to DataFrames to keep column names\n",
    "X_train_scaled_et = pd.DataFrame(X_train_scaled_et, columns=X_et.columns)\n",
    "X_val_scaled_et = pd.DataFrame(X_val_scaled_et, columns=X_et.columns)\n",
    "X_test_scaled_et = pd.DataFrame(X_test_scaled_et, columns=X_et.columns)\n",
    "\n",
    "# 4. Train model on training data with default hyperparameters\n",
    "log_reg_et = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_et.fit(X_train_scaled_et, y_train_et.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluate on validation data\n",
    "y_val_pred_et = log_reg_et.predict(X_val_scaled_et)\n",
    "val_accuracy_et = accuracy_score(y_val_et, y_val_pred_et)\n",
    "print(f\"Validation Accuracy (ET party): {val_accuracy_et:.4f}\")\n",
    "print(\"\\nValidation Classification Report (ET party):\")\n",
    "print(classification_report(y_val_et, y_val_pred_et))\n",
    "\n",
    "# 6. Train final model on combined train+validation data\n",
    "# Combine train and validation data\n",
    "X_train_val_scaled_et = pd.concat([X_train_scaled_et, X_val_scaled_et])\n",
    "y_train_val_combined_et = np.concatenate((y_train_et, y_val_et))\n",
    "\n",
    "# Train final model\n",
    "final_model_et = LogisticRegression(max_iter=1000, random_state=42)\n",
    "final_model_et.fit(X_train_val_scaled_et, y_train_val_combined_et)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Evaluate on test data\n",
    "y_test_pred_et = final_model_et.predict(X_test_scaled_et)\n",
    "test_accuracy_et = accuracy_score(y_test_et, y_test_pred_et)\n",
    "print(f\"\\nTest Accuracy (ET party): {test_accuracy_et:.4f}\")\n",
    "print(\"\\nTest Classification Report (ET party):\")\n",
    "print(classification_report(y_test_et, y_test_pred_et))\n",
    "print(\"\\nConfusion Matrix (ET party):\")\n",
    "print(confusion_matrix(y_test_et, y_test_pred_et))\n",
    "\n",
    "# 8. Feature importance\n",
    "feature_importance_et = pd.DataFrame({\n",
    "    'Feature': X_et.columns,\n",
    "    'Importance': np.abs(final_model_et.coef_[0])\n",
    "})\n",
    "feature_importance_et = feature_importance_et.sort_values('Importance', ascending=False)\n",
    "print(\"\\nFeature Importance (ET party):\")\n",
    "print(feature_importance_et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Senator or Representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for logistic model\n",
    "# ------\n",
    "# Target\n",
    "lb = LabelBinarizer()\n",
    "y_et_pos = engineered_features['position'] = lb.fit_transform(engineered_features['position'])\n",
    "\n",
    "# Features\n",
    "\n",
    "X_et_pos_prep = engineered_features.drop(['position'], axis = 1)\n",
    "X_et_pos = pd.get_dummies(X_et_pos_prep)\n",
    "X_et_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split data into train+validation and test sets (80/20)\n",
    "X_train_val_et_pos, X_test_et_pos, y_train_val_et_pos, y_test_et_pos = train_test_split(\n",
    "    X_et_pos, y_et_pos, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Further split train+validation into train and validation sets (75/25, resulting in 60/20/20 overall)\n",
    "X_train_et_pos, X_val_et_pos, y_train_et_pos, y_val_et_pos = train_test_split(\n",
    "    X_train_val_et_pos, y_train_val_et_pos, test_size=0.25, random_state=42)\n",
    "\n",
    "# 3. Standardize each set separately, fitting only on training data\n",
    "scaler_et_pos = StandardScaler()\n",
    "X_train_scaled_et_pos = scaler_et_pos.fit_transform(X_train_et_pos)\n",
    "X_val_scaled_et_pos = scaler_et_pos.transform(X_val_et_pos)  # Use training params\n",
    "X_test_scaled_et_pos = scaler_et_pos.transform(X_test_et_pos)  # Use training params\n",
    "\n",
    "# Convert back to DataFrames to keep column names\n",
    "X_train_scaled_et_pos = pd.DataFrame(X_train_scaled_et_pos, columns=X_et_pos.columns)\n",
    "X_val_scaled_et_pos = pd.DataFrame(X_val_scaled_et_pos, columns=X_et_pos.columns)\n",
    "X_test_scaled_et_pos = pd.DataFrame(X_test_scaled_et_pos, columns=X_et_pos.columns)\n",
    "\n",
    "# 4. Train model on training data with default hyperparameters\n",
    "log_reg_et_pos = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_et_pos.fit(X_train_scaled_et_pos, y_train_et_pos.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluate on validation data\n",
    "y_val_pred_et_pos = log_reg_et_pos.predict(X_val_scaled_et_pos)\n",
    "val_accuracy_et_pos = accuracy_score(y_val_et_pos, y_val_pred_et_pos)\n",
    "print(f\"Validation Accuracy (ET Position): {val_accuracy_et_pos:.4f}\")\n",
    "print(\"\\nValidation Classification Report (ET Position):\")\n",
    "print(classification_report(y_val_et_pos, y_val_pred_et_pos))\n",
    "\n",
    "# 6. Train final model on combined train+validation data\n",
    "# Combine train and validation data\n",
    "X_train_val_scaled_et_pos = pd.concat([X_train_scaled_et_pos, X_val_scaled_et_pos])\n",
    "y_train_val_combined_et_pos = np.concatenate((y_train_et_pos, y_val_et_pos))\n",
    "\n",
    "# Train final model\n",
    "final_model_et_pos = LogisticRegression(max_iter=1000, random_state=42)\n",
    "final_model_et_pos.fit(X_train_val_scaled_et_pos, y_train_val_combined_et_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Evaluate on test data\n",
    "y_test_pred_et_pos = final_model_et_pos.predict(X_test_scaled_et_pos)\n",
    "test_accuracy_et_pos = accuracy_score(y_test_et_pos, y_test_pred_et_pos)\n",
    "print(f\"\\nTest Accuracy (ET position): {test_accuracy_et:.4f}\")\n",
    "print(\"\\nTest Classification Report (ET position):\")\n",
    "print(classification_report(y_test_et_pos, y_test_pred_et_pos))\n",
    "print(\"\\nConfusion Matrix (ET position):\")\n",
    "print(confusion_matrix(y_test_et_pos, y_test_pred_et_pos))\n",
    "\n",
    "# 8. Feature importance\n",
    "feature_importance_et_pos = pd.DataFrame({\n",
    "    'Feature': X_et_pos.columns,\n",
    "    'Importance': np.abs(final_model_et_pos.coef_[0])\n",
    "})\n",
    "feature_importance_et_pos = feature_importance_et_pos.sort_values('Importance', ascending=False)\n",
    "print(\"\\nFeature Importance (ET party):\")\n",
    "print(feature_importance_et_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to predict Democrat or Republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create X and y feature sets for both prediction tasks\n",
    "# First for party prediction (Democrat or Republican)\n",
    "X_tfidf = tfidf_df.copy()  # Copy the TF-IDF dataframe for features\n",
    "\n",
    "# Create target variable for party prediction\n",
    "lb_party = LabelBinarizer()\n",
    "y_party_tfidf = lb_party.fit_transform(congress_tweets_sample['party'])\n",
    "\n",
    "# MODEL 1: DEMOCRAT OR REPUBLICAN PREDICTION\n",
    "# 2. Split data into train+validation and test sets (80/20)\n",
    "X_train_val_party, X_test_party, y_train_val_party, y_test_party = train_test_split(\n",
    "    X_tfidf, y_party_tfidf, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Further split train+validation into train and validation sets (75/25)\n",
    "X_train_party, X_val_party, y_train_party, y_val_party = train_test_split(\n",
    "    X_train_val_party, y_train_val_party, test_size=0.25, random_state=42)\n",
    "\n",
    "# 4. Standardize each set separately, fitting only on training data\n",
    "scaler_party = StandardScaler()\n",
    "X_train_scaled_party = scaler_party.fit_transform(X_train_party)\n",
    "X_val_scaled_party = scaler_party.transform(X_val_party)\n",
    "X_test_scaled_party = scaler_party.transform(X_test_party)\n",
    "\n",
    "# Convert back to DataFrames to keep column names\n",
    "X_train_scaled_party = pd.DataFrame(X_train_scaled_party, columns=X_tfidf.columns)\n",
    "X_val_scaled_party = pd.DataFrame(X_val_scaled_party, columns=X_tfidf.columns)\n",
    "X_test_scaled_party = pd.DataFrame(X_test_scaled_party, columns=X_tfidf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train model on training data\n",
    "log_reg_party = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# Convert one-hot encoded y_train_party to class indices\n",
    "y_train_party_indices = np.argmax(y_train_party, axis=1)\n",
    "log_reg_party.fit(X_train_scaled_party, y_train_party_indices)\n",
    "\n",
    "# 6. Evaluate on validation data\n",
    "y_val_pred_party = log_reg_party.predict(X_val_scaled_party)\n",
    "y_val_party_indices = np.argmax(y_val_party, axis=1)\n",
    "val_accuracy_party = accuracy_score(y_val_party_indices, y_val_pred_party)\n",
    "print(f\"Validation Accuracy (Party): {val_accuracy_party:.4f}\")\n",
    "print(\"\\nValidation Classification Report (Party):\")\n",
    "print(classification_report(y_val_party_indices, y_val_pred_party))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train_scaled_party shape: {X_train_scaled_party.shape}\")\n",
    "print(f\"y_train_party shape before ravel: {y_train_party.shape}\")\n",
    "print(f\"y_train_party shape after ravel: {y_train_party.ravel().shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train final model on combined train+validation data\n",
    "X_train_val_scaled_party = pd.concat([X_train_scaled_party, X_val_scaled_party])\n",
    "y_train_val_combined_party = np.concatenate((y_train_party, y_val_party))\n",
    "\n",
    "# Train final party model\n",
    "final_model_party = LogisticRegression(max_iter=1000, random_state=42)\n",
    "y_train_val_combined_indices = np.argmax(y_train_val_combined_party, axis=1)\n",
    "final_model_party.fit(X_train_val_scaled_party, y_train_val_combined_indices)\n",
    "\n",
    "# 8. Evaluate on test data\n",
    "y_test_pred_party = final_model_party.predict(X_test_scaled_party)\n",
    "# Convert test labels to indices\n",
    "y_test_party_indices = np.argmax(y_test_party, axis=1)\n",
    "test_accuracy_party = accuracy_score(y_test_party_indices, y_test_pred_party)\n",
    "print(f\"\\nTest Accuracy (Party): {test_accuracy_party:.4f}\")\n",
    "print(\"\\nTest Classification Report (Party):\")\n",
    "print(classification_report(y_test_party_indices, y_test_pred_party))\n",
    "print(\"\\nConfusion Matrix (Party):\")\n",
    "print(confusion_matrix(y_test_party_indices, y_test_pred_party))\n",
    "\n",
    "# 9. Feature importance (top 20 most important features)\n",
    "feature_importance_party = pd.DataFrame({\n",
    "    'Feature': X_tfidf.columns,\n",
    "    'Importance': np.abs(final_model_party.coef_[0])\n",
    "})\n",
    "feature_importance_party = feature_importance_party.sort_values('Importance', ascending=False)\n",
    "print(\"\\nTop 20 Feature Importance (Party):\")\n",
    "print(feature_importance_party.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_tfidf shape:\", X_tfidf.shape)\n",
    "print(\"congress_tweets_sample shape:\", congress_tweets_sample.shape)\n",
    "print(\"y_party_tfidf shape:\", y_party_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to predict Senator or Representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable for position prediction (Senator or Representative)\n",
    "lb_position = LabelBinarizer()\n",
    "y_position_tfidf = lb_position.fit_transform(congress_tweets_sample['position'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train+validation and test sets (80/20)\n",
    "X_train_val_position, X_test_position, y_train_val_position, y_test_position = train_test_split(\n",
    "    X_tfidf, y_position_tfidf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split train+validation into train and validation sets (75/25)\n",
    "X_train_position, X_val_position, y_train_position, y_val_position = train_test_split(\n",
    "    X_train_val_position, y_train_val_position, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize each set separately, fitting only on training data\n",
    "scaler_position = StandardScaler()\n",
    "X_train_scaled_position = scaler_position.fit_transform(X_train_position)\n",
    "X_val_scaled_position = scaler_position.transform(X_val_position)\n",
    "X_test_scaled_position = scaler_position.transform(X_test_position)\n",
    "\n",
    "# Convert back to DataFrames to keep column names\n",
    "X_train_scaled_position = pd.DataFrame(X_train_scaled_position, columns=X_tfidf.columns)\n",
    "X_val_scaled_position = pd.DataFrame(X_val_scaled_position, columns=X_tfidf.columns)\n",
    "X_test_scaled_position = pd.DataFrame(X_test_scaled_position, columns=X_tfidf.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on training data\n",
    "log_reg_position = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# Convert one-hot encoded labels to indices if needed\n",
    "y_train_position_indices = np.argmax(y_train_position, axis=1) if y_train_position.shape[1] > 1 else y_train_position\n",
    "log_reg_position.fit(X_train_scaled_position, y_train_position_indices)\n",
    "\n",
    "# Evaluate on validation data\n",
    "y_val_pred_position = log_reg_position.predict(X_val_scaled_position)\n",
    "# Convert validation labels to indices if needed\n",
    "y_val_position_indices = np.argmax(y_val_position, axis=1) if y_val_position.shape[1] > 1 else y_val_position\n",
    "val_accuracy_position = accuracy_score(y_val_position_indices, y_val_pred_position)\n",
    "print(f\"Validation Accuracy (Position): {val_accuracy_position:.4f}\")\n",
    "print(\"\\nValidation Classification Report (Position):\")\n",
    "print(classification_report(y_val_position_indices, y_val_pred_position))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on combined train+validation data\n",
    "X_train_val_scaled_position = pd.concat([X_train_scaled_position, X_val_scaled_position])\n",
    "# Concatenate y labels\n",
    "y_train_val_combined_position = np.concatenate((y_train_position, y_val_position))\n",
    "y_train_val_combined_position_indices = np.argmax(y_train_val_combined_position, axis=1) if y_train_val_combined_position.shape[1] > 1 else y_train_val_combined_position\n",
    "\n",
    "# Train final position model\n",
    "final_model_position = LogisticRegression(max_iter=1000, random_state=42)\n",
    "final_model_position.fit(X_train_val_scaled_position, y_train_val_combined_position_indices)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_test_pred_position = final_model_position.predict(X_test_scaled_position)\n",
    "# Convert test labels to indices if needed\n",
    "y_test_position_indices = np.argmax(y_test_position, axis=1) if y_test_position.shape[1] > 1 else y_test_position\n",
    "test_accuracy_position = accuracy_score(y_test_position_indices, y_test_pred_position)\n",
    "print(f\"\\nTest Accuracy (Position): {test_accuracy_position:.4f}\")\n",
    "print(\"\\nTest Classification Report (Position):\")\n",
    "print(classification_report(y_test_position_indices, y_test_pred_position))\n",
    "print(\"\\nConfusion Matrix (Position):\")\n",
    "print(confusion_matrix(y_test_position_indices, y_test_pred_position))\n",
    "\n",
    "# Feature importance for position prediction\n",
    "feature_importance_position = pd.DataFrame({\n",
    "    'Feature': X_tfidf.columns,\n",
    "    'Importance': np.abs(final_model_position.coef_[0])\n",
    "})\n",
    "feature_importance_position = feature_importance_position.sort_values('Importance', ascending=False)\n",
    "print(\"\\nTop 20 Feature Importance (Position):\")\n",
    "print(feature_importance_position.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to predict Democrat or Republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create target variable for party prediction\n",
    "lb_party = LabelBinarizer()\n",
    "y_party_embed = lb_party.fit_transform(congress_tweets_sample['party'])\n",
    "\n",
    "# 2. Split data into train+validation and test sets (80/20)\n",
    "X_train_val_party, X_test_party, y_train_val_party, y_test_party = train_test_split(\n",
    "    doc_average_embeddings, y_party_embed, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Further split train+validation into train and validation sets (75/25)\n",
    "X_train_party, X_val_party, y_train_party, y_val_party = train_test_split(\n",
    "    X_train_val_party, y_train_val_party, test_size=0.25, random_state=42)\n",
    "\n",
    "# 4. Standardize each set separately, fitting only on training data\n",
    "scaler_party_embed = StandardScaler()\n",
    "X_train_scaled_party = scaler_party_embed.fit_transform(X_train_party)\n",
    "X_val_scaled_party = scaler_party_embed.transform(X_val_party)\n",
    "X_test_scaled_party = scaler_party_embed.transform(X_test_party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train model on training data\n",
    "log_reg_party_embed = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# Convert one-hot encoded y_train_party to class indices\n",
    "y_train_party_indices = np.argmax(y_train_party, axis=1) if y_train_party.shape[1] > 1 else y_train_party\n",
    "log_reg_party_embed.fit(X_train_scaled_party, y_train_party_indices)\n",
    "\n",
    "# 6. Evaluate on validation data\n",
    "y_val_pred_party = log_reg_party_embed.predict(X_val_scaled_party)\n",
    "# Convert validation labels to indices\n",
    "y_val_party_indices = np.argmax(y_val_party, axis=1) if y_val_party.shape[1] > 1 else y_val_party\n",
    "val_accuracy_party = accuracy_score(y_val_party_indices, y_val_pred_party)\n",
    "print(f\"Validation Accuracy (Party - Word Embeddings): {val_accuracy_party:.4f}\")\n",
    "print(\"\\nValidation Classification Report (Party - Word Embeddings):\")\n",
    "print(classification_report(y_val_party_indices, y_val_pred_party))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train final model on combined train+validation data\n",
    "X_train_val_scaled_party = np.vstack([X_train_scaled_party, X_val_scaled_party])\n",
    "# Concatenate y labels and then convert to indices\n",
    "y_train_val_combined_party = np.concatenate((y_train_party, y_val_party))\n",
    "y_train_val_combined_indices = np.argmax(y_train_val_combined_party, axis=1) if y_train_val_combined_party.shape[1] > 1 else y_train_val_combined_party\n",
    "\n",
    "# Train final party model\n",
    "final_model_party_embed = LogisticRegression(max_iter=1000, random_state=42)\n",
    "final_model_party_embed.fit(X_train_val_scaled_party, y_train_val_combined_indices)\n",
    "\n",
    "# 8. Evaluate on test data\n",
    "y_test_pred_party = final_model_party_embed.predict(X_test_scaled_party)\n",
    "# Convert test labels to indices\n",
    "y_test_party_indices = np.argmax(y_test_party, axis=1) if y_test_party.shape[1] > 1 else y_test_party\n",
    "test_accuracy_party = accuracy_score(y_test_party_indices, y_test_pred_party)\n",
    "print(f\"\\nTest Accuracy (Party - Word Embeddings): {test_accuracy_party:.4f}\")\n",
    "print(\"\\nTest Classification Report (Party - Word Embeddings):\")\n",
    "print(classification_report(y_test_party_indices, y_test_pred_party))\n",
    "print(\"\\nConfusion Matrix (Party - Word Embeddings):\")\n",
    "print(confusion_matrix(y_test_party_indices, y_test_pred_party))\n",
    "\n",
    "# 9. Feature importance (word embeddings are less interpretable, but we can still check coefficients)\n",
    "if hasattr(final_model_party_embed, 'coef_'):\n",
    "    coef = final_model_party_embed.coef_[0] if final_model_party_embed.coef_.ndim > 1 else final_model_party_embed.coef_\n",
    "    # Since embeddings are not directly interpretable like TF-IDF features,\n",
    "    # we can just look at the magnitude of coefficients\n",
    "    print(\"\\nTop 10 embedding dimensions by coefficient magnitude (Party):\")\n",
    "    top_dims = np.argsort(np.abs(coef))[-10:][::-1]\n",
    "    for i, dim in enumerate(top_dims):\n",
    "        print(f\"Dimension {dim}: {coef[dim]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to predict Senator or Representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model to predict Position (Senator or Representative) ###\n",
    "\n",
    "# 1. Create target variable for position prediction\n",
    "lb_position = LabelBinarizer()\n",
    "y_position_embed = lb_position.fit_transform(congress_tweets_sample['position'])\n",
    "\n",
    "# 2. Split data into train+validation and test sets (80/20)\n",
    "X_train_val_position, X_test_position, y_train_val_position, y_test_position = train_test_split(\n",
    "    doc_average_embeddings, y_position_embed, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Further split train+validation into train and validation sets (75/25)\n",
    "X_train_position, X_val_position, y_train_position, y_val_position = train_test_split(\n",
    "    X_train_val_position, y_train_val_position, test_size=0.25, random_state=42)\n",
    "\n",
    "# 4. Standardize each set separately, fitting only on training data\n",
    "scaler_position_embed = StandardScaler()\n",
    "X_train_scaled_position = scaler_position_embed.fit_transform(X_train_position)\n",
    "X_val_scaled_position = scaler_position_embed.transform(X_val_position)\n",
    "X_test_scaled_position = scaler_position_embed.transform(X_test_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train model on training data\n",
    "log_reg_position_embed = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# Convert one-hot encoded labels to indices if needed\n",
    "y_train_position_indices = np.argmax(y_train_position, axis=1) if y_train_position.shape[1] > 1 else y_train_position\n",
    "log_reg_position_embed.fit(X_train_scaled_position, y_train_position_indices)\n",
    "\n",
    "# 6. Evaluate on validation data\n",
    "y_val_pred_position = log_reg_position_embed.predict(X_val_scaled_position)\n",
    "# Convert validation labels to indices if needed\n",
    "y_val_position_indices = np.argmax(y_val_position, axis=1) if y_val_position.shape[1] > 1 else y_val_position\n",
    "val_accuracy_position = accuracy_score(y_val_position_indices, y_val_pred_position)\n",
    "print(f\"\\nValidation Accuracy (Position - Word Embeddings): {val_accuracy_position:.4f}\")\n",
    "print(\"\\nValidation Classification Report (Position - Word Embeddings):\")\n",
    "print(classification_report(y_val_position_indices, y_val_pred_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train final model on combined train+validation data\n",
    "X_train_val_scaled_position = np.vstack([X_train_scaled_position, X_val_scaled_position])\n",
    "# Concatenate y labels\n",
    "y_train_val_combined_position = np.concatenate((y_train_position, y_val_position))\n",
    "y_train_val_combined_position_indices = np.argmax(y_train_val_combined_position, axis=1) if y_train_val_combined_position.shape[1] > 1 else y_train_val_combined_position\n",
    "\n",
    "# Train final position model\n",
    "final_model_position_embed = LogisticRegression(max_iter=1000, random_state=42)\n",
    "final_model_position_embed.fit(X_train_val_scaled_position, y_train_val_combined_position_indices)\n",
    "\n",
    "# 8. Evaluate on test data\n",
    "y_test_pred_position = final_model_position_embed.predict(X_test_scaled_position)\n",
    "# Convert test labels to indices if needed\n",
    "y_test_position_indices = np.argmax(y_test_position, axis=1) if y_test_position.shape[1] > 1 else y_test_position\n",
    "test_accuracy_position = accuracy_score(y_test_position_indices, y_test_pred_position)\n",
    "print(f\"\\nTest Accuracy (Position - Word Embeddings): {test_accuracy_position:.4f}\")\n",
    "print(\"\\nTest Classification Report (Position - Word Embeddings):\")\n",
    "print(classification_report(y_test_position_indices, y_test_pred_position))\n",
    "print(\"\\nConfusion Matrix (Position - Word Embeddings):\")\n",
    "print(confusion_matrix(y_test_position_indices, y_test_pred_position))\n",
    "\n",
    "# 9. Feature importance for position prediction (similar limitations as above)\n",
    "if hasattr(final_model_position_embed, 'coef_'):\n",
    "    coef = final_model_position_embed.coef_[0] if final_model_position_embed.coef_.ndim > 1 else final_model_position_embed.coef_\n",
    "    print(\"\\nTop 10 embedding dimensions by coefficient magnitude (Position):\")\n",
    "    top_dims = np.argsort(np.abs(coef))[-10:][::-1]\n",
    "    for i, dim in enumerate(top_dims):\n",
    "        print(f\"Dimension {dim}: {coef[dim]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision\n",
    "\n",
    "Given the results from the above 6 models, I will select word wmbedding as it performed best, especially for predicting position, with a 79% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only non-text features logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Democrat or Republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define non-text features to use\n",
    "non_text_features = [\n",
    "    'word_count',              # Total words in tweet\n",
    "    'avg_word_length',         # Average word length \n",
    "    'tweet_char',              # Character count\n",
    "    'polarity',                # TextBlob overall sentiment polarity\n",
    "    'subjectivity',            # TextBlob subjectivity score\n",
    "    # Polarity categories\n",
    "    'polarity_category_Very Positive',\n",
    "    'polarity_category_Positive',\n",
    "    'polarity_category_Neutral',\n",
    "    'polarity_category_Negative',\n",
    "    'polarity_category_Very Negative',\n",
    "    # Subjectivity categories\n",
    "    'subjectivity_category_Subjective',\n",
    "    'subjectivity_category_Neutral',\n",
    "    'subjectivity_category_Objective'\n",
    "]\n",
    "\n",
    "# Check which features are available in congress_tweets_sample\n",
    "available_non_text_features = [f for f in non_text_features if f in congress_tweets_sample.columns]\n",
    "print(f\"Using {len(available_non_text_features)} non-text features: {available_non_text_features}\")\n",
    "\n",
    "# Create target variable for party prediction\n",
    "lb_party = LabelBinarizer()\n",
    "y_party = lb_party.fit_transform(congress_tweets_sample['party'])\n",
    "\n",
    "# Split data into train+validation and test sets (80/20)\n",
    "X_train_val_party_nt, X_test_party_nt, y_train_val_party, y_test_party = train_test_split(\n",
    "    congress_tweets_sample[available_non_text_features], y_party, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split train+validation into train and validation sets (75/25)\n",
    "X_train_party_nt, X_val_party_nt, y_train_party, y_val_party = train_test_split(\n",
    "    X_train_val_party_nt, y_train_val_party, test_size=0.25, random_state=42)\n",
    "\n",
    "# Scale the non-text features\n",
    "scaler_party_nt = StandardScaler()\n",
    "X_train_scaled_party_nt = scaler_party_nt.fit_transform(X_train_party_nt)\n",
    "X_val_scaled_party_nt = scaler_party_nt.transform(X_val_party_nt)\n",
    "X_test_scaled_party_nt = scaler_party_nt.transform(X_test_party_nt)\n",
    "\n",
    "# Train model on training data\n",
    "log_reg_party_nt = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# Convert one-hot encoded y_train_party to class indices\n",
    "y_train_party_indices = np.argmax(y_train_party, axis=1) if y_train_party.shape[1] > 1 else y_train_party\n",
    "log_reg_party_nt.fit(X_train_scaled_party_nt, y_train_party_indices)\n",
    "\n",
    "# Evaluate on validation data\n",
    "y_val_pred_party_nt = log_reg_party_nt.predict(X_val_scaled_party_nt)\n",
    "# Convert validation labels to indices\n",
    "y_val_party_indices = np.argmax(y_val_party, axis=1) if y_val_party.shape[1] > 1 else y_val_party\n",
    "val_accuracy_party_nt = accuracy_score(y_val_party_indices, y_val_pred_party_nt)\n",
    "print(f\"\\nValidation Accuracy (Party - Non-Text Features Only): {val_accuracy_party_nt:.4f}\")\n",
    "print(\"\\nValidation Classification Report (Party - Non-Text Features Only):\")\n",
    "print(classification_report(y_val_party_indices, y_val_pred_party_nt))\n",
    "\n",
    "# Train final model on combined train+validation data\n",
    "X_train_val_scaled_party_nt = np.vstack([X_train_scaled_party_nt, X_val_scaled_party_nt])\n",
    "# Concatenate y labels and then convert to indices\n",
    "y_train_val_combined_party = np.concatenate((y_train_party, y_val_party))\n",
    "y_train_val_combined_indices = np.argmax(y_train_val_combined_party, axis=1) if y_train_val_combined_party.shape[1] > 1 else y_train_val_combined_party\n",
    "\n",
    "# Train final party model with non-text features\n",
    "final_model_party_nt = LogisticRegression(max_iter=1000, random_state=42)\n",
    "final_model_party_nt.fit(X_train_val_scaled_party_nt, y_train_val_combined_indices)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_test_pred_party_nt = final_model_party_nt.predict(X_test_scaled_party_nt)\n",
    "# Convert test labels to indices\n",
    "y_test_party_indices = np.argmax(y_test_party, axis=1) if y_test_party.shape[1] > 1 else y_test_party\n",
    "test_accuracy_party_nt = accuracy_score(y_test_party_indices, y_test_pred_party_nt)\n",
    "print(f\"\\nTest Accuracy (Party - Non-Text Features Only): {test_accuracy_party_nt:.4f}\")\n",
    "print(\"\\nTest Classification Report (Party - Non-Text Features Only):\")\n",
    "print(classification_report(y_test_party_indices, y_test_pred_party_nt))\n",
    "print(\"\\nConfusion Matrix (Party - Non-Text Features Only):\")\n",
    "print(confusion_matrix(y_test_party_indices, y_test_pred_party_nt))\n",
    "\n",
    "# Feature importance for non-text features\n",
    "if hasattr(final_model_party_nt, 'coef_'):\n",
    "    coef = final_model_party_nt.coef_[0] if final_model_party_nt.coef_.ndim > 1 else final_model_party_nt.coef_\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': available_non_text_features,\n",
    "        'Importance': np.abs(coef)\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    print(\"\\nTop Non-Text Feature Importance (Party):\")\n",
    "    print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Senator or Representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable for position prediction\n",
    "lb_position = LabelBinarizer()\n",
    "y_position = lb_position.fit_transform(congress_tweets_sample['position'])\n",
    "\n",
    "# Split data into train+validation and test sets (80/20)\n",
    "X_train_val_position_nt, X_test_position_nt, y_train_val_position, y_test_position = train_test_split(\n",
    "    congress_tweets_sample[available_non_text_features], y_position, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split train+validation into train and validation sets (75/25)\n",
    "X_train_position_nt, X_val_position_nt, y_train_position, y_val_position = train_test_split(\n",
    "    X_train_val_position_nt, y_train_val_position, test_size=0.25, random_state=42)\n",
    "\n",
    "# Scale the non-text features\n",
    "scaler_position_nt = StandardScaler()\n",
    "X_train_scaled_position_nt = scaler_position_nt.fit_transform(X_train_position_nt)\n",
    "X_val_scaled_position_nt = scaler_position_nt.transform(X_val_position_nt)\n",
    "X_test_scaled_position_nt = scaler_position_nt.transform(X_test_position_nt)\n",
    "\n",
    "# Train model on training data\n",
    "log_reg_position_nt = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# Convert one-hot encoded labels to indices if needed\n",
    "y_train_position_indices = np.argmax(y_train_position, axis=1) if y_train_position.shape[1] > 1 else y_train_position\n",
    "log_reg_position_nt.fit(X_train_scaled_position_nt, y_train_position_indices)\n",
    "\n",
    "# Evaluate on validation data\n",
    "y_val_pred_position_nt = log_reg_position_nt.predict(X_val_scaled_position_nt)\n",
    "# Convert validation labels to indices if needed\n",
    "y_val_position_indices = np.argmax(y_val_position, axis=1) if y_val_position.shape[1] > 1 else y_val_position\n",
    "val_accuracy_position_nt = accuracy_score(y_val_position_indices, y_val_pred_position_nt)\n",
    "print(f\"\\nValidation Accuracy (Position - Non-Text Features Only): {val_accuracy_position_nt:.4f}\")\n",
    "print(\"\\nValidation Classification Report (Position - Non-Text Features Only):\")\n",
    "print(classification_report(y_val_position_indices, y_val_pred_position_nt))\n",
    "\n",
    "# Train final model on combined train+validation data\n",
    "X_train_val_scaled_position_nt = np.vstack([X_train_scaled_position_nt, X_val_scaled_position_nt])\n",
    "# Concatenate y labels\n",
    "y_train_val_combined_position = np.concatenate((y_train_position, y_val_position))\n",
    "y_train_val_combined_position_indices = np.argmax(y_train_val_combined_position, axis=1) if y_train_val_combined_position.shape[1] > 1 else y_train_val_combined_position\n",
    "\n",
    "# Train final position model with non-text features\n",
    "final_model_position_nt = LogisticRegression(max_iter=1000, random_state=42)\n",
    "final_model_position_nt.fit(X_train_val_scaled_position_nt, y_train_val_combined_position_indices)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_test_pred_position_nt = final_model_position_nt.predict(X_test_scaled_position_nt)\n",
    "# Convert test labels to indices if needed\n",
    "y_test_position_indices = np.argmax(y_test_position, axis=1) if y_test_position.shape[1] > 1 else y_test_position\n",
    "test_accuracy_position_nt = accuracy_score(y_test_position_indices, y_test_pred_position_nt)\n",
    "print(f\"\\nTest Accuracy (Position - Non-Text Features Only): {test_accuracy_position_nt:.4f}\")\n",
    "print(\"\\nTest Classification Report (Position - Non-Text Features Only):\")\n",
    "print(classification_report(y_test_position_indices, y_test_pred_position_nt))\n",
    "print(\"\\nConfusion Matrix (Position - Non-Text Features Only):\")\n",
    "print(confusion_matrix(y_test_position_indices, y_test_pred_position_nt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined features (word embeddings and non-text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Democrat or Republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine word embeddings with non-text features\n",
    "# First get the non-text features and scale them\n",
    "non_text_features_scaled = scaler_party_nt.fit_transform(congress_tweets_sample[available_non_text_features])\n",
    "\n",
    "# Use the same train/val/test split indices as before\n",
    "tweet_indices_train_val, tweet_indices_test = train_test_split(\n",
    "    range(len(congress_tweets_sample)), test_size=0.2, random_state=42)\n",
    "tweet_indices_train, tweet_indices_val = train_test_split(\n",
    "    tweet_indices_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Split the doc_average_embeddings and non-text features\n",
    "embeddings_train = doc_average_embeddings[tweet_indices_train]\n",
    "embeddings_val = doc_average_embeddings[tweet_indices_val] \n",
    "embeddings_test = doc_average_embeddings[tweet_indices_test]\n",
    "\n",
    "non_text_train = non_text_features_scaled[tweet_indices_train]\n",
    "non_text_val = non_text_features_scaled[tweet_indices_val]\n",
    "non_text_test = non_text_features_scaled[tweet_indices_test]\n",
    "\n",
    "# Combine features using numpy hstack (horizontal stack)\n",
    "X_combined_train_party = np.hstack([embeddings_train, non_text_train])\n",
    "X_combined_val_party = np.hstack([embeddings_val, non_text_val])\n",
    "X_combined_test_party = np.hstack([embeddings_test, non_text_test])\n",
    "\n",
    "# Get corresponding labels\n",
    "y_party_train_indices = np.argmax(y_party[tweet_indices_train], axis=1) if y_party.ndim > 1 and y_party.shape[1] > 1 else y_party[tweet_indices_train]\n",
    "y_party_val_indices = np.argmax(y_party[tweet_indices_val], axis=1) if y_party.ndim > 1 and y_party.shape[1] > 1 else y_party[tweet_indices_val]\n",
    "y_party_test_indices = np.argmax(y_party[tweet_indices_test], axis=1) if y_party.ndim > 1 and y_party.shape[1] > 1 else y_party[tweet_indices_test]\n",
    "\n",
    "# Train party model with combined features\n",
    "log_reg_party_combined = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_party_combined.fit(X_combined_train_party, y_party_train_indices)\n",
    "\n",
    "# Evaluate on validation data\n",
    "y_val_pred_party_combined = log_reg_party_combined.predict(X_combined_val_party)\n",
    "val_accuracy_party_combined = accuracy_score(y_party_val_indices, y_val_pred_party_combined)\n",
    "print(f\"\\nValidation Accuracy (Party - Combined Features): {val_accuracy_party_combined:.4f}\")\n",
    "print(\"\\nValidation Classification Report (Party - Combined Features):\")\n",
    "print(classification_report(y_party_val_indices, y_val_pred_party_combined))\n",
    "\n",
    "# Train final model on combined train+validation data\n",
    "X_combined_train_val_party = np.vstack([X_combined_train_party, X_combined_val_party])\n",
    "y_party_train_val_indices = np.concatenate([y_party_train_indices, y_party_val_indices])\n",
    "\n",
    "# Train final combined party model\n",
    "final_model_party_combined = LogisticRegression(max_iter=1000, random_state=42)\n",
    "final_model_party_combined.fit(X_combined_train_val_party, y_party_train_val_indices)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_test_pred_party_combined = final_model_party_combined.predict(X_combined_test_party)\n",
    "test_accuracy_party_combined = accuracy_score(y_party_test_indices, y_test_pred_party_combined)\n",
    "print(f\"\\nTest Accuracy (Party - Combined Features): {test_accuracy_party_combined:.4f}\")\n",
    "print(\"\\nTest Classification Report (Party - Combined Features):\")\n",
    "print(classification_report(y_party_test_indices, y_test_pred_party_combined))\n",
    "print(\"\\nConfusion Matrix (Party - Combined Features):\")\n",
    "print(confusion_matrix(y_party_test_indices, y_test_pred_party_combined))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Senator or Representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get corresponding position labels\n",
    "y_position_train_indices = np.argmax(y_position[tweet_indices_train], axis=1) if y_position.ndim > 1 and y_position.shape[1] > 1 else y_position[tweet_indices_train]\n",
    "y_position_val_indices = np.argmax(y_position[tweet_indices_val], axis=1) if y_position.ndim > 1 and y_position.shape[1] > 1 else y_position[tweet_indices_val]\n",
    "y_position_test_indices = np.argmax(y_position[tweet_indices_test], axis=1) if y_position.ndim > 1 and y_position.shape[1] > 1 else y_position[tweet_indices_test]\n",
    "\n",
    "# Train position model with combined features\n",
    "log_reg_position_combined = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_position_combined.fit(X_combined_train_party, y_position_train_indices)  # Reuse the same combined features\n",
    "\n",
    "# Evaluate on validation data\n",
    "y_val_pred_position_combined = log_reg_position_combined.predict(X_combined_val_party)\n",
    "val_accuracy_position_combined = accuracy_score(y_position_val_indices, y_val_pred_position_combined)\n",
    "print(f\"\\nValidation Accuracy (Position - Combined Features): {val_accuracy_position_combined:.4f}\")\n",
    "print(\"\\nValidation Classification Report (Position - Combined Features):\")\n",
    "print(classification_report(y_position_val_indices, y_val_pred_position_combined))\n",
    "\n",
    "# Train final model on combined train+validation data\n",
    "X_combined_train_val_position = X_combined_train_val_party  # Reuse the same combined features\n",
    "y_position_train_val_indices = np.concatenate([y_position_train_indices, y_position_val_indices])\n",
    "\n",
    "# Train final combined position model\n",
    "final_model_position_combined = LogisticRegression(max_iter=1000, random_state=42)\n",
    "final_model_position_combined.fit(X_combined_train_val_position, y_position_train_val_indices)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_test_pred_position_combined = final_model_position_combined.predict(X_combined_test_party)\n",
    "test_accuracy_position_combined = accuracy_score(y_position_test_indices, y_test_pred_position_combined)\n",
    "print(f\"\\nTest Accuracy (Position - Combined Features): {test_accuracy_position_combined:.4f}\")\n",
    "print(\"\\nTest Classification Report (Position - Combined Features):\")\n",
    "print(classification_report(y_position_test_indices, y_test_pred_position_combined))\n",
    "print(\"\\nConfusion Matrix (Position - Combined Features):\")\n",
    "print(confusion_matrix(y_position_test_indices, y_test_pred_position_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL COMPARISON SUMMARY\n",
    "print(\"\\n----- MODEL PERFORMANCE COMPARISON -----\")\n",
    "print(\"\\nParty Prediction Models:\")\n",
    "print(f\"Word Embeddings Only: {0.655:.4f}\")  \n",
    "print(f\"Non-Text Features Only: {test_accuracy_party_nt:.4f}\")\n",
    "print(f\"Combined Features: {test_accuracy_party_combined:.4f}\")\n",
    "\n",
    "print(\"\\nPosition Prediction Models:\")\n",
    "print(f\"Word Embeddings Only: {0.790:.4f}\")  \n",
    "print(f\"Non-Text Features Only: {test_accuracy_position_nt:.4f}\")\n",
    "print(f\"Combined Features: {test_accuracy_position_combined:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why do standard preprocessing techniques need to be further customized to a particular corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because datasets will have their own quirks. For example, in this dataset I had to remove @ signs, \"RT\", and \"QT\", and I was only able to identify those quirks after doing initial exploratory analysis, like word clouds. A different corpus will have different abbreviations or nuances that may not be addressed by standard preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Did you find evidence for the idea that Democrats and Republicans have different sentiments in their tweets? What about Senators and Representatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did not find major evidence that Demcrats and Republicans have different sentiments. Democrats aseemed to have more spread in their sentiment while Republicans seemed to be slightly more clustered around neutral sentiment. For position, Senators had a slightly higher average polarity (o.136) than Representatives (0.113). In general, it seems that senators seem to tweet with slightly more positive sentiment and subjectivity than Representatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Why is validating your exploratory and unsupervised learning approaches with a supervised learning algorithm valuable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using supervised methods helps confirm any patterns that I noticed using unsupervised methods and perhaps most helpfully, supervised methods provide me with concrete metrics like accuracy and F1 score to evaluate if the features are actually useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Did text only, non-text only, or text and non-text features together perform the best? What is the intuition behind combining text and non-text features in a supervised learning algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For position prediction, combined features and non-text both performed the same, but were slightly higher than word embeddings. For Party prediction, word embeddings only performed slightly better than the other two. The intution behind combining text and non-text features in a supervised learning algorithm allows us to leverage information that can be complementary. Text features might capture language patterns while non-text features might get at structura or contextual information. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
